# *
# * Modify this file and rename it as config.yaml
# *
system:
  ssl: false
  debug: true
  port: 8000
  timeout: 30
  # ! this is just for temporary testing, to ease on-the-fly switching between keys
  # ! In production the proxy key should ONLY be set in the environment variable MOOLTIPROXY_KEY
  masterkey: XXXXXXXXXXXXXXXXXXXXX 

cerbere:
  # maximum number of tries before blocking an ip address (optional, default to 3)
  # The ban last for all the proxy life. To unban, restart the proxy (sorry)
  max_tries: 5
  # IP address Whitelist and blacklist (optional)
  whitelist: [] # a list of ip addresses
  blacklist: [] # a list of ip addresses

targets:
  # Name of the target to use by default if no target is specified in request header (mandatory)
  # Must match the name of one of the targets defined below
  default: openai

  # List of targets
  # * url is mandatory and is the base url of the target
  #
  # * if an api key is needed to acces the target you should provide either a key or and envkey:
  #   - if a key is provided, the value of the key will be used as the api key
  #   - if an envkey is provided, the value of the "envkey" environment variable will be used as the api key
  #   - if both are provided, the key value will be used as the api key, and the envkey will be ignored
  # !   In production, only use envkey!!!
  #
  # * preserveMethod: during http permanent redirections (301), POST method is normally converted to GET, which may be a problem
  # *                 if the target endpoint only supports POST. If preserveMethod is set to true,
  # *                 the original method is preserved (optional, default to false).
  #
  # * mapping is mandatory and defines the supported incoming endpoints:
  #   - Each mapping entry defines how to map an incoming endpoint path
  #   - endpoints not defined here will be rejected with a 404 HTTP error
  #   - Each mapping entry can be:
  #     - empty: the incoming endpoint path will be used as the target endpoint path, with no transcoding (pass through)
  #     - a string: the incoming endpoint path will be mapped to the target endpoint path, with no transcoding
  #     - an object, with the following properties:
  #       - path: the target endpoint path to use (optional, default to the incoming endpoint path)
  #       - in: the transcoding to apply to the incoming request body (optional, default to identity)
  #       - out: the transcoding to apply to the target response body (optional, default to identity)
  #       - prompter: the prompter.function to use to transform a list of messages into a string (optional, default to no prompter)
  #       - template: the prompt template to use (optional, default to no template, disregarded if prompter does not use templates)
  #
  
  # * Exemple with OpenAI official API, where it's just a pass through
  openai: 
    url: https://api.openai.com/v1
    envkey: YOUR_OPENAI_API_KEY_ENVIRONMENT_VARIABLE_NAME
    mapping:
      /:
      /models: 
      /completions:
      /chat/completions:

  # * An example with Hugging Face TGI serving a Llama2-Chat model on runpod
  runpod1:
    url: https://your_runpod_machine-80.proxy.runpod.net
    preserveMethod: true  # needed for runpod
    mapping:
      # These are pass-throughs if you still want to provide the original endpoints
      # Delete them if you don't need them    
      /health:
      /info:
      /:
      /generate:
      /generate_stream:
      /metrics:

      # These are the OpenAI endpoints that will be exposed by the proxy
      # and transcoded to and from TGI
      /completions:
        path: /generate
        in: textReqOpenAItoTGI
        out: textAnsTGItoOpenAI

      /chat/completions:
        path: /generate
        in: chatReqOpenAItoTGI
        out: chatAnsTGItoOpenAI
        prompter: llama2_chat

  # * An example with Hugging Face TGI serving a Llama2 (no chat) model
  target1:
    url: http://your_url:port
    mapping:
      # These are pass-throughs if you still want to provide the original endpoints
      # Delete them if you don't need them    
      /health:
      /info:
      /:
      /generate:
      /generate_stream:
      /metrics:

      # These are the OpenAI endpoints that will be exposed by the proxy
      # and transcoded to and from TGI
      /completions:
        path: /generate
        in: textReqOpenAItoTGI
        out: textAnsTGItoOpenAI

      /chat/completions:
        path: /generate
        in: chatReqOpenAItoTGI
        out: chatAnsTGItoOpenAI
        prompter: fromTemplate
        template: HumanResponse

  # * An example with Hugging Face TGI serving a Falcon Instruct model
  target2:
    url: http://your_url:port
    mapping:
      # These are pass-throughs if you still want to provide the original endpoints
      # Delete them if you don't need them
      /health:
      /info:
      /:
      /generate:
      /generate_stream:
      /metrics:

      # These are the OpenAI endpoints that will be exposed by the proxy
      # and transcoded to and from TGI
      /completions:
        path: /generate
        in: textReqOpenAItoTGI
        out: textAnsTGItoOpenAI

      /chat/completions:
        path: /generate
        in: chatReqOpenAItoTGI
        out: chatAnsTGItoOpenAI
        prompter: fromTemplate
        template: FalconInstruct


# list of prompt templates to transform a list of prompts 
# with associated roles into a text completion input
# i.e. a single string input with custom separators
templates:
  HumanResponse:
    preprompt: ""
    start: ""
    system: ""
    user: "\n### HUMAN:\n"
    assistant: "\n### RESPONSE:\n"

  FalconInstruct:
    preprompt: ""
    start: ""
    system: ""
    user: ">>>QUESTION<<<"
    assistant: ">>>ANSWER<<<"    